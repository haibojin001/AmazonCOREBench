# Controlling Output Rankings in Generative Engines for LLM-based Search

This repository demonstrates how **CORE** controls *output rankings* of LLM-generated product recommendations through **prompt interventions**.

The current demo focuses on **Memory Foam Pillows**, showing how different suffix styles â€” **Review-based** and **Reasoning-based** â€” affect the ranking behavior of large language models.

---

## ğŸ› ï¸ Environment Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/AmazonCOREBench.git
cd AmazonCOREBench-Demo
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

## ğŸ“¦ Data Preparation

```bash
python scraper.py
```

This script saves product listings as a .jsonl file, where each line is one product object.

## ğŸ’¡ Demo: Controlling Output Ranking

The demo.py script demonstrates how prompt suffixes can control model outputs.

It simulates an Amazon-like recommendation query:

â€œIâ€™m looking to buy a memory foam pillow. Which models are best?â€

and appends one of two suffix styles:

ğŸ“ Review Suffix â€” mimics real-world reviews and experiential tone.

ğŸ§© Reasoning Suffix â€” adds analytical reasoning and structured comparison.

The outputs are written into Markdown files to visualize the modelâ€™s ranking shift.

## ğŸš€ Run the Demo

```bash
python demo.py --input "./memory foam pillow.jsonl" --out ./out
```

With OpenAI API (real inference)


```bash
export OPENAI_API_KEY="your_api_key_here"
python demo.py --input "./memory foam pillow.jsonl" --out ./out --model gpt-4o
```

```bash
out/
â”œâ”€â”€ memory_foam_pillow_review.md      
â”œâ”€â”€ memory_foam_pillow_reasoning.md   

```
This repository serves as a miniature example of the CORE. Stay tuned for iterative updates

